# -*- coding: utf-8 -*-
"""PHT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DjSjMlEBFA1TK7d2clIUE9t62JXXiGJv
"""

!pip install transformers datasets sentencepiece rouge-score openpyxl
!pip install torch torchvision torchaudio

import numpy as np
import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments
from datasets import Dataset
from rouge_score import rouge_scorer

dataset_path = "/content/drive/MyDrive/The_Hindu_paper_dataset_2015_to_2025.xlsx"
df = pd.read_excel(dataset_path)
df.head()

df = df.fillna('')
df = df.drop_duplicates()
df['Content'] = df['Content'].astype(str)
df['Headline'] = df['Headline'].astype(str)
df = df.reset_index(drop = True)
print("Preprocessing complete. Dataframe info after preprocessing:")
df.info()

def split_dataset(df, train_size = 0.8, val_size = 0.1, test_size = 0.1):
  if(train_size + val_size + test_size != 1.0):
    raise ValueError("The sum of train_size, val_size, and test_size must be 1.0")
  train_df , temp_df = train_test_split(df, test_size = (val_size + test_size), random_state = 42)
  adjusted_test_size = test_size/(test_size+val_size)
  val_df, test_df = train_test_split(df, test_size = adjusted_test_size, random_state = 42)
  return train_df, val_df, test_df

train_df, test_df, val_df = split_dataset(df)
print("Training set size:", len(train_df))
print("Validation set size:", len(val_df))
print("Testing set size:", len(test_df))

tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')
model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
max_input_length = 512
max_target_length = 128

def tokenize_data(tokenizer, dataframe, max_length):
    inputs = tokenizer(
        dataframe["Content"],
        padding="max_length",
        truncation=True,
        max_length=max_length,
        return_tensors="pt",
    )
    labels = tokenizer(
        dataframe["Headline"],
        padding="max_length",
        truncation=True,
        max_length=max_length,
        return_tensors="pt",
    )
    inputs['labels'] = labels['input_ids']
    return inputs

train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)

train_dataset = train_dataset.map(lambda x: tokenize_data(tokenizer, x, max_input_length), batched=True)
val_dataset = val_dataset.map(lambda x: tokenize_data(tokenizer, x, max_input_length), batched=True)

print(train_dataset)

training_args = TrainingArguments(
    output_dir = './bart_hindu_results',
    eval_strategy = 'epoch',
    save_strategy = 'epoch',
    per_device_train_batch_size = 2,
    per_device_eval_batch_size = 2,
    learning_rate = 5e-5,
    num_train_epochs = 10,
    weight_decay = 0.01,
    logging_steps = 50,
    logging_dir = './bart_hindu_logs',
    fp16=torch.cuda.is_available(),
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer
)

trainer.train()

